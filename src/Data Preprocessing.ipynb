{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Preprocessing libraries\n",
    "import preprocessor as p\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from datetime import datetime, timedelta \n",
    "#For the regular expression\n",
    "import itertools\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save filepath to variable for easier access\n",
    "# tweets_file_path = os.path.expanduser('~/Downloads/covid19_tweets.csv')\n",
    "# # read the data and store data \n",
    "# tweets_data = pd.read_csv(tweets_file_path)\n",
    "\n",
    "# data_subset = tweets_data.head(500)\n",
    "# # tweets_data['text']\n",
    "# tweets_data.describe()    \n",
    "# data_subset.to_csv (r'~/Downloads/export_dataframe.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filepath to variable for easier access\n",
    "tweets_file_path = 'C:/Users/ayush/Documents/Trento/Data Mining/Project/data/covid19_tweets_sort.csv'\n",
    "# read the data and store data \n",
    "tweets_data = pd.read_csv(tweets_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 12:11:07\n",
      "2020-07-25 12:27:21\n",
      "0 days 00:00:01\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "set_of_dates = sorted(set(tweets_data.date))\n",
    "print(set_of_dates[0])\n",
    "print(set_of_dates[-1])\n",
    "\n",
    "date= pd.to_datetime(set_of_dates)\n",
    "min_time_delta= timedelta(days = 0, hours=0, minutes=0, seconds=0)\n",
    "# print(set_of_dates[1]-set_of_dates[0])\n",
    "# print((set_of_dates[1]-set_of_dates[0])>min_time_delta)\n",
    "print((date[1]-date[0]))\n",
    "print((date[1]-date[0])>min_time_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicates\n",
    "tweets_data.drop_duplicates(inplace=True, subset=\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(tweets_data['text']):\n",
    "    tweets_data.loc[i,'text_clean'] = p.clean(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    " #Removes Numbers\n",
    "    data = data.astype(str).str.replace('\\d+', '')\n",
    "    lower_text = data.str.lower()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer =  TweetTokenizer()\n",
    " \n",
    "    def lemmatize_text(text):\n",
    "        return [(lemmatizer.lemmatize(w)) for w \\\n",
    "                       in w_tokenizer.tokenize((text))]\n",
    "    def remove_punctuation(words):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "            if new_word != '' and len(new_word) > 2:\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    words = lower_text.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text_clean\n",
      "0       [some, more, excellent, work, from, our, assis...\n",
      "1       [donald, trump, hold, telerally, campaign, fir...\n",
      "2       [all, play, part, keeping, our, community, saf...\n",
      "3       [hello, kindly, help, the, child, your, consti...\n",
      "4       [contemplate, sometimes, all, you, have, for, ...\n",
      "...                                                   ...\n",
      "179103  [hearing, lot, about, what, great, piece, this...\n",
      "179104  [day, having, group, session, under, the, sess...\n",
      "179105  [just, wondering, the, result, test, will, del...\n",
      "179106  [while, the, whole, country, divided, between,...\n",
      "179107  [have, updated, our, guide, now, include, coro...\n",
      "\n",
      "[179108 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "pre_tweets = preprocess_data(tweets_data['text_clean'])\n",
    "print(pre_tweets)\n",
    "tweets_data['text_clean'] = pre_tweets\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tweets_data['text_clean'] = tweets_data['text_clean'].apply(lambda x: [item for item in \\\n",
    "                                    x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\anaconda3\\lib\\site-packages\\ekphrasis\\classes\\exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "a = []\n",
    "for i in range(len(tweets_data)):\n",
    "#     if tweets_data['text_clean'][i] != a and pd.notna(tweets_data['text_clean'][i]):\n",
    "    for elem in tweets_data['text_clean'][i]:\n",
    "        listToStr1 = ' '.join([(elem) for elem in tweets_data['text_clean'][i]])\n",
    "#     print(listToStr1)\n",
    "    tweets_data.loc[i,'Segmented#'] = seg_tw.segment(listToStr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stand', 'may', 'empty', 'biggest', 'fan', 'cheering', 'home', 'opening', 'day', 'grandfat']\n"
     ]
    }
   ],
   "source": [
    "print(tweets_data['text_clean'][9])\n",
    "# tweets_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    excellent  work  assistant  director  jennifer...\n",
       "1    donald   trump  hold telerally  campaign  firs...\n",
       "2    play  part keeping  community  safe help slow ...\n",
       "3    hello  kindly help child  constituency  contin...\n",
       "4    contemplate  sometimes swim  meditating  nappi...\n",
       "5    year  parent excited  getting  internet  conne...\n",
       "6    week  commission  passed  revised local  healt...\n",
       "7    opponent  doesnt care thats  that s  tweet vot...\n",
       "8    chief  public health  officer  warns pandemic ...\n",
       "9    stand  may empty  biggest  fan  cheering  home...\n",
       "Name: segmented, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['segmented']=tweets_data['Segmented#']\n",
    "tweets_data['segmented'][0:10]\n",
    "# tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                          \n",
      "1                                                          \n",
      "2                                                          \n",
      "3                                                          \n",
      "4                                                          \n",
      "                                ...                        \n",
      "179103                                          ['covid19']\n",
      "179104       ['shapersummit20', 'skills', 'designthinking']\n",
      "179105                                          ['covid19']\n",
      "179106    ['ssrcase', 'mann_ki_nahi_students_ki_baat', '...\n",
      "179107                                       ['athomecare']\n",
      "Name: hashtags, Length: 179108, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert hashtags to lowercase\n",
    "tweets_data['hashtags'] = tweets_data.hashtags.str.lower()\n",
    "\n",
    "tweets_data['hashtags'] = tweets_data.hashtags.fillna('')\n",
    "\n",
    "print(tweets_data['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_data['hash']=tweets_data['hashtags#']\n",
    "tweets_data['hashtags'].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Segmented#</th>\n",
       "      <th>segmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Center on Trauma and Adversity</td>\n",
       "      <td>Cleveland, OH</td>\n",
       "      <td>Reducing the impact of #trauma and #adversity ...</td>\n",
       "      <td>26-10-2017 18:48</td>\n",
       "      <td>6292</td>\n",
       "      <td>446</td>\n",
       "      <td>3619</td>\n",
       "      <td>False</td>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>Some more excellent work from our Assistant Di...</td>\n",
       "      <td></td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>[excellent, work, assistant, director, jennife...</td>\n",
       "      <td>excellent  work  assistant  director  jennifer...</td>\n",
       "      <td>excellent  work  assistant  director  jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IAM Platform</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>Curation | Tools | Tips | Services\\n\\nIAM Plat...</td>\n",
       "      <td>13-12-2010 21:24</td>\n",
       "      <td>17816</td>\n",
       "      <td>14941</td>\n",
       "      <td>65440</td>\n",
       "      <td>False</td>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>Donald Trump holds ‘tele-rally’ in campaign fi...</td>\n",
       "      <td></td>\n",
       "      <td>IAMBLOG2TWITTER</td>\n",
       "      <td>False</td>\n",
       "      <td>[donald, trump, hold, telerally, campaign, fir...</td>\n",
       "      <td>donald   trump  hold telerally  campaign  firs...</td>\n",
       "      <td>donald   trump  hold telerally  campaign  firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA Business, Consumer Services and Housing Agency</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "      <td>We are responsible for licensing and oversight...</td>\n",
       "      <td>15-10-2012 18:10</td>\n",
       "      <td>1351</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>False</td>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>We all play a part in keeping our communities ...</td>\n",
       "      <td></td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "      <td>[play, part, keeping, community, safe, help, s...</td>\n",
       "      <td>play  part keeping  community  safe help slow ...</td>\n",
       "      <td>play  part keeping  community  safe help slow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enjuba</td>\n",
       "      <td>East Africa</td>\n",
       "      <td>We are a children’s education organisation aim...</td>\n",
       "      <td>27-02-2013 13:59</td>\n",
       "      <td>10260</td>\n",
       "      <td>6291</td>\n",
       "      <td>4831</td>\n",
       "      <td>False</td>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>Hello @onyukandrew1 Kindly help the children i...</td>\n",
       "      <td></td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>[hello, kindly, help, child, constituency, con...</td>\n",
       "      <td>hello  kindly help child  constituency  contin...</td>\n",
       "      <td>hello  kindly help child  constituency  contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jordi Ortega</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Story miner: Innovation, Ventures @StateFarm. ...</td>\n",
       "      <td>15-11-2013 21:57</td>\n",
       "      <td>1475</td>\n",
       "      <td>4417</td>\n",
       "      <td>14809</td>\n",
       "      <td>False</td>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>“CONTEMPLATE.” Sometimes, all you have to do i...</td>\n",
       "      <td></td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>False</td>\n",
       "      <td>[contemplate, sometimes, swim, meditating, nap...</td>\n",
       "      <td>contemplate  sometimes swim  meditating  nappi...</td>\n",
       "      <td>contemplate  sometimes swim  meditating  nappi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_name    user_location  \\\n",
       "0                     Center on Trauma and Adversity    Cleveland, OH   \n",
       "1                                       IAM Platform        Worldwide   \n",
       "2  CA Business, Consumer Services and Housing Agency   Sacramento, CA   \n",
       "3                                             enjuba      East Africa   \n",
       "4                                       Jordi Ortega  Los Angeles, CA   \n",
       "\n",
       "                                    user_description      user_created  \\\n",
       "0  Reducing the impact of #trauma and #adversity ...  26-10-2017 18:48   \n",
       "1  Curation | Tools | Tips | Services\\n\\nIAM Plat...  13-12-2010 21:24   \n",
       "2  We are responsible for licensing and oversight...  15-10-2012 18:10   \n",
       "3  We are a children’s education organisation aim...  27-02-2013 13:59   \n",
       "4  Story miner: Innovation, Ventures @StateFarm. ...  15-11-2013 21:57   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0            6292           446             3619          False   \n",
       "1           17816         14941            65440          False   \n",
       "2            1351           278              278          False   \n",
       "3           10260          6291             4831          False   \n",
       "4            1475          4417            14809          False   \n",
       "\n",
       "               date                                               text  \\\n",
       "0  24-07-2020 23:47  Some more excellent work from our Assistant Di...   \n",
       "1  24-07-2020 23:47  Donald Trump holds ‘tele-rally’ in campaign fi...   \n",
       "2  24-07-2020 23:47  We all play a part in keeping our communities ...   \n",
       "3  24-07-2020 23:47  Hello @onyukandrew1 Kindly help the children i...   \n",
       "4  24-07-2020 23:47  “CONTEMPLATE.” Sometimes, all you have to do i...   \n",
       "\n",
       "  hashtags              source  is_retweet  \\\n",
       "0           Twitter for iPhone       False   \n",
       "1              IAMBLOG2TWITTER       False   \n",
       "2              Twitter Web App       False   \n",
       "3           Twitter for iPhone       False   \n",
       "4           Twitter for iPhone       False   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [excellent, work, assistant, director, jennife...   \n",
       "1  [donald, trump, hold, telerally, campaign, fir...   \n",
       "2  [play, part, keeping, community, safe, help, s...   \n",
       "3  [hello, kindly, help, child, constituency, con...   \n",
       "4  [contemplate, sometimes, swim, meditating, nap...   \n",
       "\n",
       "                                          Segmented#  \\\n",
       "0  excellent  work  assistant  director  jennifer...   \n",
       "1  donald   trump  hold telerally  campaign  firs...   \n",
       "2  play  part keeping  community  safe help slow ...   \n",
       "3  hello  kindly help child  constituency  contin...   \n",
       "4  contemplate  sometimes swim  meditating  nappi...   \n",
       "\n",
       "                                           segmented  \n",
       "0  excellent  work  assistant  director  jennifer...  \n",
       "1  donald   trump  hold telerally  campaign  firs...  \n",
       "2  play  part keeping  community  safe help slow ...  \n",
       "3  hello  kindly help child  constituency  contin...  \n",
       "4  contemplate  sometimes swim  meditating  nappi...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>segmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[excellent, work, assistant, director, jennife...</td>\n",
       "      <td></td>\n",
       "      <td>excellent  work  assistant  director  jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[donald, trump, hold, telerally, campaign, fir...</td>\n",
       "      <td></td>\n",
       "      <td>donald   trump  hold telerally  campaign  firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[play, part, keeping, community, safe, help, s...</td>\n",
       "      <td></td>\n",
       "      <td>play  part keeping  community  safe help slow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[hello, kindly, help, child, constituency, con...</td>\n",
       "      <td></td>\n",
       "      <td>hello  kindly help child  constituency  contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[contemplate, sometimes, swim, meditating, nap...</td>\n",
       "      <td></td>\n",
       "      <td>contemplate  sometimes swim  meditating  nappi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date                                         text_clean  \\\n",
       "0  24-07-2020 23:47  [excellent, work, assistant, director, jennife...   \n",
       "1  24-07-2020 23:47  [donald, trump, hold, telerally, campaign, fir...   \n",
       "2  24-07-2020 23:47  [play, part, keeping, community, safe, help, s...   \n",
       "3  24-07-2020 23:47  [hello, kindly, help, child, constituency, con...   \n",
       "4  24-07-2020 23:47  [contemplate, sometimes, swim, meditating, nap...   \n",
       "\n",
       "  hashtags                                          segmented  \n",
       "0           excellent  work  assistant  director  jennifer...  \n",
       "1           donald   trump  hold telerally  campaign  firs...  \n",
       "2           play  part keeping  community  safe help slow ...  \n",
       "3           hello  kindly help child  constituency  contin...  \n",
       "4           contemplate  sometimes swim  meditating  nappi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets_data = tweets_data[['date', 'text_clean', 'hashtags','segmented']].copy()\n",
    "clean_tweets_data = pd.DataFrame([tweets_data.date, tweets_data.text_clean, tweets_data.hashtags, tweets_data.segmented]).transpose()\n",
    "clean_tweets_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_tweets_data['merged_text'] = clean_tweets_data['text_clean'].astype(str) + clean_tweets_data['hashtags'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>segmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[excellent, work, assistant, director, jennife...</td>\n",
       "      <td></td>\n",
       "      <td>excellent  work  assistant  director  jennifer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[donald, trump, hold, telerally, campaign, fir...</td>\n",
       "      <td></td>\n",
       "      <td>donald   trump  hold telerally  campaign  firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[play, part, keeping, community, safe, help, s...</td>\n",
       "      <td></td>\n",
       "      <td>play  part keeping  community  safe help slow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[hello, kindly, help, child, constituency, con...</td>\n",
       "      <td></td>\n",
       "      <td>hello  kindly help child  constituency  contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>[contemplate, sometimes, swim, meditating, nap...</td>\n",
       "      <td></td>\n",
       "      <td>contemplate  sometimes swim  meditating  nappi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date                                         text_clean  \\\n",
       "0  24-07-2020 23:47  [excellent, work, assistant, director, jennife...   \n",
       "1  24-07-2020 23:47  [donald, trump, hold, telerally, campaign, fir...   \n",
       "2  24-07-2020 23:47  [play, part, keeping, community, safe, help, s...   \n",
       "3  24-07-2020 23:47  [hello, kindly, help, child, constituency, con...   \n",
       "4  24-07-2020 23:47  [contemplate, sometimes, swim, meditating, nap...   \n",
       "\n",
       "  hashtags                                          segmented  \n",
       "0           excellent  work  assistant  director  jennifer...  \n",
       "1           donald   trump  hold telerally  campaign  firs...  \n",
       "2           play  part keeping  community  safe help slow ...  \n",
       "3           hello  kindly help child  constituency  contin...  \n",
       "4           contemplate  sometimes swim  meditating  nappi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_array(text):\n",
    "    str1=text.replace(']','').replace('[','')\n",
    "    l=str1.replace(\"'\",'').split(\",\")\n",
    "    l1 = [item.strip(' ') for item in l]\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(clean_tweets_data['text_clean'].astype(str)):\n",
    "        clean_tweets_data['text_clean'][i] = (str_to_array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(clean_tweets_data['hashtags'].astype(str)):\n",
    "        clean_tweets_data['hashtags'][i] = (str_to_array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_data['merged_text'] = clean_tweets_data['text_clean'] + clean_tweets_data['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [excellent, work, assistant, director, jennife...\n",
      "1         [donald, trump, hold, telerally, campaign, fir...\n",
      "2         [play, part, keeping, community, safe, help, s...\n",
      "3         [hello, kindly, help, child, constituency, con...\n",
      "4         [contemplate, sometimes, swim, meditating, nap...\n",
      "                                ...                        \n",
      "179103    [hearing, lot, great, piece, timpsons, handled...\n",
      "179104    [day, group, session, session, using, create, ...\n",
      "179105    [wondering, result, test, delayed, passed, alr...\n",
      "179106    [whole, country, divided, increasing, ssrcase,...\n",
      "179107    [updated, guide, include, coronvirus, section,...\n",
      "Name: merged_text, Length: 179108, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clean_tweets_data['merged_text'])\n",
    "clean_tweets_data['segmented'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'work', 'assistant', 'director', 'jennifer', 'king', 'informed', 'decision', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clean_tweets_data['merged_text'][0])\n",
    "clean_tweets_data['merged_text'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = lambda x: 'word_{}'.format(x + 1)\n",
    "# new_df = pd.concat([pd.DataFrame(\n",
    "#     clean_tweets_data.merged_text.values.tolist(),\n",
    "#     clean_tweets_data.index, dtype=object).fillna('').rename(columns=f),\n",
    "#     clean_tweets_data.date],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 'word_{}'.format(x + 1)\n",
    "new_df = (pd.DataFrame\n",
    "    (clean_tweets_data.merged_text.values.tolist(),\n",
    "     clean_tweets_data.index, dtype=object).fillna('').rename(columns=f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>word_10</th>\n",
       "      <th>word_11</th>\n",
       "      <th>word_12</th>\n",
       "      <th>word_13</th>\n",
       "      <th>word_14</th>\n",
       "      <th>word_15</th>\n",
       "      <th>word_16</th>\n",
       "      <th>word_17</th>\n",
       "      <th>word_18</th>\n",
       "      <th>word_19</th>\n",
       "      <th>word_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent</td>\n",
       "      <td>work</td>\n",
       "      <td>assistant</td>\n",
       "      <td>director</td>\n",
       "      <td>jennifer</td>\n",
       "      <td>king</td>\n",
       "      <td>informed</td>\n",
       "      <td>decision</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>donald</td>\n",
       "      <td>trump</td>\n",
       "      <td>hold</td>\n",
       "      <td>telerally</td>\n",
       "      <td>campaign</td>\n",
       "      <td>first</td>\n",
       "      <td>amid</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>read</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>play</td>\n",
       "      <td>part</td>\n",
       "      <td>keeping</td>\n",
       "      <td>community</td>\n",
       "      <td>safe</td>\n",
       "      <td>help</td>\n",
       "      <td>slow</td>\n",
       "      <td>spread</td>\n",
       "      <td>protect</td>\n",
       "      <td>others</td>\n",
       "      <td>weekend</td>\n",
       "      <td>wearing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hello</td>\n",
       "      <td>kindly</td>\n",
       "      <td>help</td>\n",
       "      <td>child</td>\n",
       "      <td>constituency</td>\n",
       "      <td>continue</td>\n",
       "      <td>learning</td>\n",
       "      <td>getting</td>\n",
       "      <td>boo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contemplate</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>swim</td>\n",
       "      <td>meditating</td>\n",
       "      <td>napping</td>\n",
       "      <td>pagoda</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179103</th>\n",
       "      <td>hearing</td>\n",
       "      <td>lot</td>\n",
       "      <td>great</td>\n",
       "      <td>piece</td>\n",
       "      <td>timpsons</td>\n",
       "      <td>handled</td>\n",
       "      <td>unfortu</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179104</th>\n",
       "      <td>day</td>\n",
       "      <td>group</td>\n",
       "      <td>session</td>\n",
       "      <td>session</td>\n",
       "      <td>using</td>\n",
       "      <td>create</td>\n",
       "      <td>shapersummit20</td>\n",
       "      <td>skills</td>\n",
       "      <td>designthinking</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179105</th>\n",
       "      <td>wondering</td>\n",
       "      <td>result</td>\n",
       "      <td>test</td>\n",
       "      <td>delayed</td>\n",
       "      <td>passed</td>\n",
       "      <td>alreadyid</td>\n",
       "      <td>like</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179106</th>\n",
       "      <td>whole</td>\n",
       "      <td>country</td>\n",
       "      <td>divided</td>\n",
       "      <td>increasing</td>\n",
       "      <td>ssrcase</td>\n",
       "      <td>mann_ki_nahi_students_ki_baat</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179107</th>\n",
       "      <td>updated</td>\n",
       "      <td>guide</td>\n",
       "      <td>include</td>\n",
       "      <td>coronvirus</td>\n",
       "      <td>section</td>\n",
       "      <td>download</td>\n",
       "      <td>free</td>\n",
       "      <td>guide</td>\n",
       "      <td>athomecare</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179108 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_1     word_2     word_3      word_4        word_5  \\\n",
       "0         excellent       work  assistant    director      jennifer   \n",
       "1            donald      trump       hold   telerally      campaign   \n",
       "2              play       part    keeping   community          safe   \n",
       "3             hello     kindly       help       child  constituency   \n",
       "4       contemplate  sometimes       swim  meditating       napping   \n",
       "...             ...        ...        ...         ...           ...   \n",
       "179103      hearing        lot      great       piece      timpsons   \n",
       "179104          day      group    session     session         using   \n",
       "179105    wondering     result       test     delayed        passed   \n",
       "179106        whole    country    divided  increasing       ssrcase   \n",
       "179107      updated      guide    include  coronvirus       section   \n",
       "\n",
       "                               word_6          word_7       word_8  \\\n",
       "0                                king        informed     decision   \n",
       "1                               first            amid  coronavirus   \n",
       "2                                help            slow       spread   \n",
       "3                            continue        learning      getting   \n",
       "4                              pagoda                                \n",
       "...                               ...             ...          ...   \n",
       "179103                        handled         unfortu      covid19   \n",
       "179104                         create  shapersummit20       skills   \n",
       "179105                      alreadyid            like      covid19   \n",
       "179106  mann_ki_nahi_students_ki_baat         covid19                \n",
       "179107                       download            free        guide   \n",
       "\n",
       "                word_9 word_10  word_11  word_12 word_13 word_14 word_15  \\\n",
       "0                                                                          \n",
       "1             pandemic    read                                             \n",
       "2              protect  others  weekend  wearing                           \n",
       "3                  boo                                                     \n",
       "4                                                                          \n",
       "...                ...     ...      ...      ...     ...     ...     ...   \n",
       "179103                                                                     \n",
       "179104  designthinking                                                     \n",
       "179105                                                                     \n",
       "179106                                                                     \n",
       "179107      athomecare                                                     \n",
       "\n",
       "       word_16 word_17 word_18 word_19 word_20  \n",
       "0                                               \n",
       "1                                               \n",
       "2                                               \n",
       "3                                               \n",
       "4                                               \n",
       "...        ...     ...     ...     ...     ...  \n",
       "179103                                          \n",
       "179104                                          \n",
       "179105                                          \n",
       "179106                                          \n",
       "179107                                          \n",
       "\n",
       "[179108 rows x 20 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([clean_tweets_data.date, new_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_11</th>\n",
       "      <th>word_12</th>\n",
       "      <th>word_13</th>\n",
       "      <th>word_14</th>\n",
       "      <th>word_15</th>\n",
       "      <th>word_16</th>\n",
       "      <th>word_17</th>\n",
       "      <th>word_18</th>\n",
       "      <th>word_19</th>\n",
       "      <th>word_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>excellent</td>\n",
       "      <td>work</td>\n",
       "      <td>assistant</td>\n",
       "      <td>director</td>\n",
       "      <td>jennifer</td>\n",
       "      <td>king</td>\n",
       "      <td>informed</td>\n",
       "      <td>decision</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>donald</td>\n",
       "      <td>trump</td>\n",
       "      <td>hold</td>\n",
       "      <td>telerally</td>\n",
       "      <td>campaign</td>\n",
       "      <td>first</td>\n",
       "      <td>amid</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>pandemic</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>play</td>\n",
       "      <td>part</td>\n",
       "      <td>keeping</td>\n",
       "      <td>community</td>\n",
       "      <td>safe</td>\n",
       "      <td>help</td>\n",
       "      <td>slow</td>\n",
       "      <td>spread</td>\n",
       "      <td>protect</td>\n",
       "      <td>...</td>\n",
       "      <td>weekend</td>\n",
       "      <td>wearing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>hello</td>\n",
       "      <td>kindly</td>\n",
       "      <td>help</td>\n",
       "      <td>child</td>\n",
       "      <td>constituency</td>\n",
       "      <td>continue</td>\n",
       "      <td>learning</td>\n",
       "      <td>getting</td>\n",
       "      <td>boo</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24-07-2020 23:47</td>\n",
       "      <td>contemplate</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>swim</td>\n",
       "      <td>meditating</td>\n",
       "      <td>napping</td>\n",
       "      <td>pagoda</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179103</th>\n",
       "      <td>30-08-2020 09:07</td>\n",
       "      <td>hearing</td>\n",
       "      <td>lot</td>\n",
       "      <td>great</td>\n",
       "      <td>piece</td>\n",
       "      <td>timpsons</td>\n",
       "      <td>handled</td>\n",
       "      <td>unfortu</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179104</th>\n",
       "      <td>30-08-2020 09:07</td>\n",
       "      <td>day</td>\n",
       "      <td>group</td>\n",
       "      <td>session</td>\n",
       "      <td>session</td>\n",
       "      <td>using</td>\n",
       "      <td>create</td>\n",
       "      <td>shapersummit20</td>\n",
       "      <td>skills</td>\n",
       "      <td>designthinking</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179105</th>\n",
       "      <td>30-08-2020 09:07</td>\n",
       "      <td>wondering</td>\n",
       "      <td>result</td>\n",
       "      <td>test</td>\n",
       "      <td>delayed</td>\n",
       "      <td>passed</td>\n",
       "      <td>alreadyid</td>\n",
       "      <td>like</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179106</th>\n",
       "      <td>30-08-2020 09:07</td>\n",
       "      <td>whole</td>\n",
       "      <td>country</td>\n",
       "      <td>divided</td>\n",
       "      <td>increasing</td>\n",
       "      <td>ssrcase</td>\n",
       "      <td>mann_ki_nahi_students_ki_baat</td>\n",
       "      <td>covid19</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179107</th>\n",
       "      <td>30-08-2020 09:07</td>\n",
       "      <td>updated</td>\n",
       "      <td>guide</td>\n",
       "      <td>include</td>\n",
       "      <td>coronvirus</td>\n",
       "      <td>section</td>\n",
       "      <td>download</td>\n",
       "      <td>free</td>\n",
       "      <td>guide</td>\n",
       "      <td>athomecare</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179108 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date       word_1     word_2     word_3      word_4  \\\n",
       "0       24-07-2020 23:47    excellent       work  assistant    director   \n",
       "1       24-07-2020 23:47       donald      trump       hold   telerally   \n",
       "2       24-07-2020 23:47         play       part    keeping   community   \n",
       "3       24-07-2020 23:47        hello     kindly       help       child   \n",
       "4       24-07-2020 23:47  contemplate  sometimes       swim  meditating   \n",
       "...                  ...          ...        ...        ...         ...   \n",
       "179103  30-08-2020 09:07      hearing        lot      great       piece   \n",
       "179104  30-08-2020 09:07          day      group    session     session   \n",
       "179105  30-08-2020 09:07    wondering     result       test     delayed   \n",
       "179106  30-08-2020 09:07        whole    country    divided  increasing   \n",
       "179107  30-08-2020 09:07      updated      guide    include  coronvirus   \n",
       "\n",
       "              word_5                         word_6          word_7  \\\n",
       "0           jennifer                           king        informed   \n",
       "1           campaign                          first            amid   \n",
       "2               safe                           help            slow   \n",
       "3       constituency                       continue        learning   \n",
       "4            napping                         pagoda                   \n",
       "...              ...                            ...             ...   \n",
       "179103      timpsons                        handled         unfortu   \n",
       "179104         using                         create  shapersummit20   \n",
       "179105        passed                      alreadyid            like   \n",
       "179106       ssrcase  mann_ki_nahi_students_ki_baat         covid19   \n",
       "179107       section                       download            free   \n",
       "\n",
       "             word_8          word_9  ...  word_11  word_12 word_13 word_14  \\\n",
       "0          decision                  ...                                     \n",
       "1       coronavirus        pandemic  ...                                     \n",
       "2            spread         protect  ...  weekend  wearing                   \n",
       "3           getting             boo  ...                                     \n",
       "4                                    ...                                     \n",
       "...             ...             ...  ...      ...      ...     ...     ...   \n",
       "179103      covid19                  ...                                     \n",
       "179104       skills  designthinking  ...                                     \n",
       "179105      covid19                  ...                                     \n",
       "179106                               ...                                     \n",
       "179107        guide      athomecare  ...                                     \n",
       "\n",
       "       word_15 word_16 word_17 word_18 word_19 word_20  \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4                                                       \n",
       "...        ...     ...     ...     ...     ...     ...  \n",
       "179103                                                  \n",
       "179104                                                  \n",
       "179105                                                  \n",
       "179106                                                  \n",
       "179107                                                  \n",
       "\n",
       "[179108 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv (r'C:/Users/ayush/Documents/Trento/Data Mining/Project/data/final_df_space_remove.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
