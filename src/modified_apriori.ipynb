{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import collections\n",
    "import collections.abc\n",
    "import numbers\n",
    "import typing\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "from collections import defaultdict\n",
    "from dataclasses import field, dataclass\n",
    "from datetimerange import DateTimeRange\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating list of unique words\n",
    "def unique_words(data):\n",
    "    all_words=[]\n",
    "\n",
    "    for column in data.columns[1:]:\n",
    "        all_words.extend(data[column].unique().tolist())\n",
    "    all_words=list(set(all_words))\n",
    "    clean_words= [x for x in all_words if pd.notna(x)]\n",
    "    #print(list(set(all_words)))\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function foe generating L1\n",
    "def level_one_frequent(data_map, local_support, minthd1, minthd2):\n",
    "    #convert int to timedelta for comparision\n",
    "    minthd1= timedelta(minthd1)\n",
    "    minthd2= timedelta(minthd2)\n",
    "    #Gen list of unique words item sets of size 1\n",
    "    all_words=unique_words(data_map)\n",
    "    \n",
    "    n= len(all_words)\n",
    "    tp= [[] for i in range(n)]\n",
    "    supp=[[] for i in range(n)]\n",
    "    #initialize all counters to zero\n",
    "    \n",
    "    last_seen=[0]*n\n",
    "    first_seen=[0]*n\n",
    "    icount=[0]*n\n",
    "    ctcount=[0]*n\n",
    "    ptcount=[0]*n\n",
    "    # map of item to time instances\n",
    "    L1={}\n",
    "    supp1_map={}\n",
    "    for index, row in data_map.iterrows():\n",
    "        #print(\"row[0]\",row[0])\n",
    "        time_stamp=pd.to_datetime(row[0],dayfirst=True)\n",
    "        transaction=list(row[1:])\n",
    "        #if len(transaction)!=0:\n",
    "        l=0\n",
    "        #iterate over each word in the set of unique words\n",
    "        for k, word in enumerate(all_words):\n",
    "            #iterate through every transaction\n",
    "            if word in transaction:\n",
    "                #check last seen\n",
    "                if(last_seen[k]==0):\n",
    "                        last_seen[k]=first_seen[k]=time_stamp\n",
    "                        icount[k]=ptcount[k]=ctcount[k]=1\n",
    "                #If difference less than minthd1 increase the count\n",
    "                elif(abs(time_stamp-last_seen[k])<minthd1):\n",
    "                    last_seen[k]=time_stamp\n",
    "                    icount[k]+=1\n",
    "                    ctcount[k]+=1\n",
    "                    ptcount[k]=ctcount[k]\n",
    "#               #else check for minthd2 and support\n",
    "                else:\n",
    "                    if((abs(last_seen[k]-first_seen[k])>=minthd2) and ((icount[k]/ptcount[k]*100)>=local_support)):\n",
    "#                         \n",
    "                        tp[k].append([first_seen[k], last_seen[k]])\n",
    "                        supp[k].append([\"{:.2f}\".format(icount[k]/ptcount[k]*100)])\n",
    "                    #reset the count and times    \n",
    "                    icount[k]=ptcount[k]=ctcount[k]=1\n",
    "                    last_seen[k]=first_seen[k]=time_stamp\n",
    "\n",
    "            else:\n",
    "                #increment transaction count for the particular time interval\n",
    "                ctcount[k]+=1\n",
    "    #iterate through all the words to record time spans and supports\n",
    "    for k in range(len(all_words)):\n",
    "        if(last_seen[k]!=0 and first_seen[k]!=0):\n",
    "            if((abs(last_seen[k]-first_seen[k])>=minthd2) and ((icount[k]/ptcount[k]*100)>=local_support)):\n",
    "                tp[k].append([first_seen[k], last_seen[k]])\n",
    "                supp[k].append([\"{:.2f}\".format(icount[k]/ptcount[k]*100)])\n",
    "                \n",
    "            if(len(tp[k]) !=0):\n",
    "                L1[all_words[k]]=tp[k]\n",
    "            if(len(supp[k])!=0):\n",
    "                supp1_map[all_words[k]]=supp[k]\n",
    "                \n",
    "                \n",
    "    return L1, supp1_map\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting the intersection between 2 time stamps andtheir comparision with minthd2 \n",
    "def valid_time_intersection(t1,t2,minthd2):\n",
    "    minthd2= timedelta(minthd2)\n",
    "    intersection_range=[]\n",
    "    for datetm1 in t1:\n",
    "        #print('datetm1',datetm1)\n",
    "        for datetm2 in t2:\n",
    "            #print('datetm2',datetm2)\n",
    "            range1=DateTimeRange(datetm1[0],datetm1[1])\n",
    "            range2=DateTimeRange(datetm2[0],datetm2[1])\n",
    "            #if intersection value exist and it is >= minthd2 then only add to list\n",
    "            if(range1.is_intersection(range2) and range1.intersection(range2).timedelta>=minthd2):\n",
    "                time_range_list=[]\n",
    "                #print('timedelta',range1.intersection(range2).timedelta)\n",
    "                #find intersection\n",
    "                time_range = range1.intersection(range2)\n",
    "                #convert to string\n",
    "                time_range.start_time_format = time_range.end_time_format= \"%Y-%m-%d %H:%M:%S\"\n",
    "                time_range.end_time_format = time_range.end_time_format= \"%Y-%m-%d %H:%M:%S\"\n",
    "                #convert back to the pd format\n",
    "                start_time=pd.to_datetime(time_range.get_start_time_str())\n",
    "                end_time=pd.to_datetime(time_range.get_end_time_str())\n",
    "                # add to the list\n",
    "                time_range_list=[start_time, end_time]    \n",
    "                #print('time_range_list',time_range_list)\n",
    "                #add to the list of all the possible intersections\n",
    "                intersection_range.append(time_range_list)\n",
    "                \n",
    "    return intersection_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate all the possible itemsets of size 2 from the given list of itemsets\n",
    "def join_step(itemsets: typing.List[tuple], length):\n",
    "    retrn_list=[]\n",
    "    all_elem=[]\n",
    "    for item in itemsets:\n",
    "        all_elem.append(item[0])\n",
    "    #print(all_elem)\n",
    "    for a, b in sorted(itertools.combinations(all_elem, 2)):\n",
    "#         print((a,) + (b,))\n",
    "        retrn_list.append((a,) + (b,))\n",
    "        \n",
    "    return retrn_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_info():\n",
    "        return 0\n",
    "#Pruning on the basis of time interval intersection. thus generating candidates of size 2\n",
    "def prune_step(L1:dict, itemsets: typing.Iterable[tuple], possible_itemsets: typing.List[tuple], minthd2:int):\n",
    "    # For faster lookups\n",
    "    \n",
    "    itemsets = set(itemsets)\n",
    "    \n",
    "    time_dict=defaultdict(time_info)\n",
    "    for possible_itemset in possible_itemsets:\n",
    "        #If time range for the item sets of length 2 is not valid then remove it from the dictionary\n",
    "        time_dict[possible_itemset]=valid_time_intersection(L1[possible_itemset[0]],L1[possible_itemset[1]],minthd2)\n",
    "        if len(time_dict[possible_itemset])==0:\n",
    "            del time_dict[possible_itemset]\n",
    "    return time_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for joining and generating possible candidates\n",
    "def apriori_gen(L1, itemsets, minthd2):\n",
    "    possible_extensions = join_step(itemsets, 2)\n",
    "    return prune_step(L1,itemsets, possible_extensions,minthd2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def level_two_frequent(Possible_itemsets, data_map, local_support, minthd1, minthd2):\n",
    "    #convert number into timedelta\n",
    "    minthd1= timedelta(minthd1)\n",
    "    minthd2= timedelta(minthd2)\n",
    "    \n",
    "    n=len(Possible_itemsets)\n",
    "    #initialize all the counts\n",
    "    tp= [[] for i in range(n)]\n",
    "    supp=[[] for i in range(n)]\n",
    "    last_seen=[0]*n\n",
    "    first_seen=[0]*n\n",
    "    icount=[0]*n\n",
    "    ctcount=[0]*n\n",
    "    ptcount=[0]*n\n",
    "    # map of item to time instances\n",
    "    L2={}\n",
    "    supp2_map={}\n",
    "    for index, row in data_map.iterrows():\n",
    "        #print(type(row))\n",
    "        \n",
    "        time_stamp=pd.to_datetime(row[0],dayfirst=True)\n",
    "        transaction=list(row[1:])\n",
    "        #for k, word in enumerate(all_words):\n",
    "        for k, Possible_itemset in enumerate(Possible_itemsets):\n",
    "            issubset = set.issubset\n",
    "            if issubset(set(Possible_itemset), transaction):\n",
    "                if(last_seen[k]==0):\n",
    "                    last_seen[k]=first_seen[k]=time_stamp\n",
    "                    icount[k]=ptcount[k]=ctcount[k]=1\n",
    "                elif(abs(time_stamp-last_seen[k])<minthd1):\n",
    "                    last_seen[k]=time_stamp\n",
    "                    icount[k]+=1\n",
    "                    ctcount[k]+=1\n",
    "                    ptcount[k]=ctcount[k]\n",
    "                else:\n",
    "                    if((abs(last_seen[k]-first_seen[k])>=minthd2) and ((icount[k]/ptcount[k]*100)>=local_support)):\n",
    "                        tp[k].append([first_seen[k], last_seen[k]])\n",
    "                        supp[k].append([\"{:.2f}\".format(icount[k]/ptcount[k]*100)])\n",
    "                        #tp.insert(k,[first_seen[k], last_seen[k]])\n",
    "                    icount[k]=ptcount[k]=ctcount[k]=1\n",
    "                    last_seen[k]=first_seen[k]=time_stamp\n",
    "                        \n",
    "            else:\n",
    "                ctcount[k]+=1\n",
    "                   \n",
    "    for k in range(len(Possible_itemsets)):\n",
    "        if(last_seen[k]!=0 and first_seen[k]!=0):\n",
    "            if((abs(last_seen[k]-first_seen[k])>=minthd2) and ((icount[k]/ptcount[k]*100)>=local_support)):\n",
    "                tp[k].append([first_seen[k], last_seen[k]])\n",
    "                supp[k].append([\"{:.2f}\".format(icount[k]/ptcount[k]*100)])\n",
    "            if(len(tp[k]) !=0):\n",
    "                L2[Possible_itemsets[k]]=tp[k]\n",
    "            if(len(supp[k])!=0):\n",
    "                supp2_map[Possible_itemsets[k]]=supp[k]\n",
    "    return L2, supp2_map\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def frequent_itemsets(data_map, local_support, minthd1, minthd2):\n",
    "    itemsets=[]\n",
    "    #construct and filter size 1 itemsets\n",
    "    L1, supp1=level_one_frequent(data_map, local_support, minthd1, minthd2)\n",
    "    for key, value in L1.items():\n",
    "        single_key=[]\n",
    "        single_key.append(key)\n",
    "        itemsets.append(tuple(single_key))\n",
    "    #construct next level candidate itemsets\n",
    "    c_k_map = apriori_gen(L1, itemsets, minthd2)\n",
    "    possible_itemsets=[key for key, value in c_k_map.items()]\n",
    "    #filter size 2 itemsets\n",
    "    L2, supp2=level_two_frequent(possible_itemsets, data_map, local_support, minthd1, minthd2)\n",
    "    #supp1 and supp2 are the maps of itemset ti their respective supports\n",
    "    return L1, supp1, L2, supp2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter number for which data to use: 1:twitter data, 2:Evaluation 1 data, 3:Evaluation 2 data, 4: twitter partial data 3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Read and Store file\n",
    "    my_path = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "    data_to_use = input(\"Please enter number for which data to use: 1:twitter data, 2:Evaluation 1 data, 3:Evaluation 2 data, 4: twitter partial data \")\n",
    "    #Specify path for i/p data\n",
    "    if data_to_use=='1':\n",
    "        file_path = os.path.join(my_path, \"../data/whole_data_processed.csv\")\n",
    "        output_path=os.path.join(my_path, \"../outputs/whole_data_processed.csv\")\n",
    "        local_support=2\n",
    "        minthd1=2\n",
    "        minthd2=4\n",
    "    elif data_to_use=='2':\n",
    "        file_path = os.path.join(my_path, \"../data/eval1.csv\")\n",
    "        output_path=os.path.join(my_path, \"../outputs/evaluation1.csv\")\n",
    "        local_support=20\n",
    "        minthd1=2\n",
    "        minthd2=3\n",
    "    elif data_to_use=='3':\n",
    "        file_path = os.path.join(my_path, \"../data/eval2.csv\")\n",
    "        output_path=os.path.join(my_path, \"../outputs/evaluation2.csv\")\n",
    "        local_support=22\n",
    "        minthd1=2\n",
    "        minthd2=3\n",
    "    elif data_to_use=='4':\n",
    "        file_path = os.path.join(my_path, \"../data/partial_data_preprocessed.csv\")\n",
    "        output_path=os.path.join(my_path, \"../outputs/partial_data_preprocessed.csv\")\n",
    "        local_support=2\n",
    "        minthd1=2\n",
    "        minthd2=4\n",
    "    else: print(\"number not valid, please re-run the script\")\n",
    "            \n",
    "    # read file and print head\n",
    "    data_map=pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "    data_map.head()\n",
    "    \n",
    "    # Parameters to edit\n",
    "#     local_support=22\n",
    "#     minthd1=2\n",
    "#     minthd2=4\n",
    "    #------------------------------------------------\n",
    "    #Running the algorithm\n",
    "    L1, supp1, L2, supp2=frequent_itemsets(data_map, local_support, minthd1, minthd2)\n",
    "    L1_set = list(L1.keys()) \n",
    "    L1_time = list(L1.values()) \n",
    "    L2_set= list(L2.keys())\n",
    "    L2_time=list(L2.values())\n",
    "\n",
    "    L1_df={'Items':L1_set, 'Time-interval':L1_time, 'Support in percent':list(supp1.values())}\n",
    "    L2_df={'Items':L2_set, 'Time-interval':L2_time, 'Support in percent':list(supp2.values())}\n",
    "    df = pd.DataFrame(L1_df)\n",
    "    df=df.append(pd.DataFrame(L2_df))\n",
    "    #print(df)\n",
    "    df.to_csv (output_path, index = False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
